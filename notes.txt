Working of the FAISS VectorDB:
When you call _vectorstore.save_local(FAISS_DIR), it creates two files:
1. index.faiss
2. index.pkl

1. index.faiss:
It will contain All your embeddings (vectors)+ The FAISS ANN (Approximate Nearest Neighbor) data structure + Optimized binary representation
In simple form it is described as â€œAll knowledge encoded as vectors + the search structure needed to quickly find similar chunks.â€

2. index.pkl:
LangChain stores metadata separately because FAISS itself cannot store:
1. document text
2. source file name
3. page number
4. chunk ID
5. embeddings mapping
6. metainfo (custom metadata)

ğŸ’¡ Why do we need both files?

During search:

FAISS looks inside index.faiss
â†’ finds vector IDs for nearest neighbors

LangChain looks inside index.pkl
â†’ retrieves the corresponding text chunks & metadata

These chunks are given to your LLM
â†’ for RAG answer generation

Without index.pkl, FAISS returns only numbers.
Without index.faiss, metadata is useless because FAISS cannot do similarity search.



***FAISS only stores the embedding vectors, not the embedding model.

embeddings is required because:

âœ” LangChain uses the embedding model object to:

1. Embed new queries  
2. Convert your userâ€™s questions into vectors  
3. Compare those vectors against index.faiss  

FAISS cannot embed text â†’ it only stores vectors.

ğŸ“˜ What happens internally when you ask a question?
1ï¸âƒ£ User asks question:
"What is the refund policy?"

2ï¸âƒ£ LangChain embeds the query using your embedding model:
query_vec = embeddings.embed_query(text)

3ï¸âƒ£ FAISS searches closest vectors:
results = faiss_index.search(query_vec)

4ï¸âƒ£ LangChain pulls matching text chunks from index.pkl

Final Summary:
| Question                                          | Answer                                              |
| ------------------------------------------------- | --------------------------------------------------- |
| â€œDo we already have all embeddings in FAISS_DIR?â€ | YES â€” but only document vectors, not query vectors  |
| â€œWhy pass embeddings to load_local?â€              | Because LangChain needs it to embed **new queries** |
| â€œWhat gets loaded from index.faiss?â€              | Vector embeddings for your document chunks          |
| â€œWhat does embeddings parameter do?â€              | Converts future user queries â†’ vectors              |


How  the recursive character text splitter works?
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    separators=["\n\n", "\n", ".", "!", "?", ",", " "],
)

ğŸ§© 1. Meaning of the Separators (order matters!)

The separators are tried from first to last:

["\n\n", "\n", ".", "!", "?", ",", " "]


This means:

1. First try to split using paragraph breaks (\n\n)
2. If that doesn't work, split by line breaks (\n)
3. If the text is still too large, split by sentence boundaries (., !, ?)
4. If still too large, split by comma
5. If still too large, split by space (word boundary)
6. As last resort, split anywhere at the character boundary (breaking a word)

This gives human-like chunks.

STEP 1 â€” Try the first separator ("\n\n")

Split at paragraph breaks

This gives a list of paragraph-sized pieces

If any paragraph is larger than 800 characters, then:

STEP 2 â€” For large pieces, try the next separator ("\n")

Split big paragraphs into lines

If any line is larger than 800 chars:

STEP 3 â€” Try next separator (".", "!", "?")

Split into sentences

If any sentence is too long:

STEP 4 â€” Try next separator (",")

Split long sentences further

If still long:

STEP 5 â€” Try space (" ")

Split at word boundaries

If even words are too long (rare):

STEP 6 â€” Fallback: split by characters

If no separator can produce chunks <= 800 chars, the splitter splits purely by character count.



Why we created HFEmbeddings?
HFEmbeddings is a wrapper class around SentenceTransformer.

LangChainâ€™s FAISS and retriever expect an embedding object that implements two methods:

embed_documents(texts) â†’ used when building the FAISS index from documents.

embed_query(text) â†’ used when embedding a user query to search against the FAISS index.

Key difference in design:

| Embedding type                    | Need to define `embed_documents`/`embed_query`? | Why                                                                  |
| --------------------------------- | ----------------------------------------------- | -------------------------------------------------------------------- |
| GoogleGenerativeAIEmbeddings      | âŒ No                                            | Already implemented inside LangChain wrapper                         |
| HuggingFace `SentenceTransformer` | âœ… Yes                                           | The raw model doesnâ€™t know LangChainâ€™s interface, so we must wrap it |

Summary:
1. HFEmbeddings is an adapter between SentenceTransformer and LangChain.
2. Googleâ€™s embedding class already implements the interface, so we didnâ€™t need to write it ourselves.
3. The methods exist because LangChain requires a standard API for embeddings: one method for documents, one for queries.

What is SentenceTransformer?

1. SentenceTransformer is a Python library that allows you to convert text into vectors (embeddings).

2. These embeddings are numerical representations of text that capture the meaning.

3. Itâ€™s built on transformer models from HuggingFace.


How is SentenceTransformer related to HuggingFace?

1. HuggingFace provides pretrained transformer models (like BERT, RoBERTa, MiniLM).

2. SentenceTransformer uses these HuggingFace models under the hood but adds an easy-to-use interface for sentence and document embeddings.

3. So you donâ€™t have to manually handle tokenization, pooling, or vector extraction â€” SentenceTransformer does it for you

Simplified relationship:
HuggingFace Transformers â†’ underlying model
SentenceTransformer â†’ wraps HuggingFace models + provides encode() for sentences
HFEmbeddings â†’ wraps SentenceTransformer + exposes LangChain interface
