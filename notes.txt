Working of the FAISS VectorDB:
When you call _vectorstore.save_local(FAISS_DIR), it creates two files:
1. index.faiss
2. index.pkl

1. index.faiss:
It will contain All your embeddings (vectors)+ The FAISS ANN (Approximate Nearest Neighbor) data structure + Optimized binary representation
In simple form it is described as â€œAll knowledge encoded as vectors + the search structure needed to quickly find similar chunks.â€

2. index.pkl:
LangChain stores metadata separately because FAISS itself cannot store:
1. document text
2. source file name
3. page number
4. chunk ID
5. embeddings mapping
6. metainfo (custom metadata)

ğŸ’¡ Why do we need both files?

During search:

FAISS looks inside index.faiss
â†’ finds vector IDs for nearest neighbors

LangChain looks inside index.pkl
â†’ retrieves the corresponding text chunks & metadata

These chunks are given to your LLM
â†’ for RAG answer generation

Without index.pkl, FAISS returns only numbers.
Without index.faiss, metadata is useless because FAISS cannot do similarity search.



***FAISS only stores the embedding vectors, not the embedding model.

embeddings is required because:

âœ” LangChain uses the embedding model object to:

1. Embed new queries  
2. Convert your userâ€™s questions into vectors  
3. Compare those vectors against index.faiss  

FAISS cannot embed text â†’ it only stores vectors.

ğŸ“˜ What happens internally when you ask a question?
1ï¸âƒ£ User asks question:
"What is the refund policy?"

2ï¸âƒ£ LangChain embeds the query using your embedding model:
query_vec = embeddings.embed_query(text)

3ï¸âƒ£ FAISS searches closest vectors:
results = faiss_index.search(query_vec)

4ï¸âƒ£ LangChain pulls matching text chunks from index.pkl

Final Summary:
| Question                                          | Answer                                              |
| ------------------------------------------------- | --------------------------------------------------- |
| â€œDo we already have all embeddings in FAISS_DIR?â€ | YES â€” but only document vectors, not query vectors  |
| â€œWhy pass embeddings to load_local?â€              | Because LangChain needs it to embed **new queries** |
| â€œWhat gets loaded from index.faiss?â€              | Vector embeddings for your document chunks          |
| â€œWhat does embeddings parameter do?â€              | Converts future user queries â†’ vectors              |


How  the recursive character text splitter works?
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    separators=["\n\n", "\n", ".", "!", "?", ",", " "],
)

ğŸ§© 1. Meaning of the Separators (order matters!)

The separators are tried from first to last:

["\n\n", "\n", ".", "!", "?", ",", " "]


This means:

1. First try to split using paragraph breaks (\n\n)
2. If that doesn't work, split by line breaks (\n)
3. If the text is still too large, split by sentence boundaries (., !, ?)
4. If still too large, split by comma
5. If still too large, split by space (word boundary)
6. As last resort, split anywhere at the character boundary (breaking a word)

This gives human-like chunks.

STEP 1 â€” Try the first separator ("\n\n")

Split at paragraph breaks

This gives a list of paragraph-sized pieces

If any paragraph is larger than 800 characters, then:

STEP 2 â€” For large pieces, try the next separator ("\n")

Split big paragraphs into lines

If any line is larger than 800 chars:

STEP 3 â€” Try next separator (".", "!", "?")

Split into sentences

If any sentence is too long:

STEP 4 â€” Try next separator (",")

Split long sentences further

If still long:

STEP 5 â€” Try space (" ")

Split at word boundaries

If even words are too long (rare):

STEP 6 â€” Fallback: split by characters

If no separator can produce chunks <= 800 chars, the splitter splits purely by character count.


